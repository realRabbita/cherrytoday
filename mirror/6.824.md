## 6.824 Comments

**Key Questions**

* What are the problems?
* What are the main features?
* What are the key techniques?
* What are the limitations & drawbacks？

#### Zookeeper

**Questions**
  how to design a stand-alone general-purpose coordination service?
  what should the API look like?
  how can other distributed applications use it?

**Features & techniques**
  ZAB consensus protocol based on Paxos
  ==linearizable writes==, writes are well-behaved, e.g. exclusive test-and-set operations
  ==FIFO client order== => “read-your-own-writes”
  a file-system-like tree of *znodes*  API
    regular, ephemeral, sequential (name + seqno)
    exclusive file creation
    mini-transactions
    watches -- avoid polling
  coordination service
    master election
    persistent master state (if state is small)
    who is the current master? (name service)
    worker registration & work queues

#### CRAQ

**Questions**
  how to design a system that can ==read from any replica== AND have ==strong consistency==?

**Features & techniques**
  Chain Replication
    head (send writes to head), chain, tail (tail responds to client)
  Chain Replication with Apportioned Queries, CRAQ
    write to head creates dirty version
    write passes through dirty version
    tail creates clean version, ACKs back along chain

**limitations & drawbacks**
  can’t handle **split brain**
  need a single "configuration manager", usually Paxos/Raft/ZK for config service


## Introduction

This is a course about infrastructure for applications.
  * Storage.
  * Communication.
  * Computation.

**The big goal**: abstractions that hide the complexity of distribution.

Topic: **performance**
  The goal: scalable throughput
    Nx servers -> Nx total throughput via parallel CPU, disk, net.
  Scaling gets harder as N grows:
    Load im-balance, stragglers, slowest-of-N latency.
    Non-parallelizable code: initialization, interaction.
    Bottlenecks from shared resources, e.g. network.
    Some performance problems aren't easily solved by scaling. e.g. quick response time for a single user request. e.g. all users want to update the same data. often requires better design rather than just more computers

Topic: **fault tolerance**
  1000s of servers, big network -> always something broken
    We'd like to hide these failures from the application.
  We often want:
    Availability -- app can make progress despite failures.
    Recoverability -- app will come back to life when failures are repaired.
  Big idea: replicated servers
    If one server crashes, can proceed using the other(s).

Topic: **consistency**
  General-purpose infrastructure needs well-defined behavior.
    E.g. "Get(k) yields the value from the most recent Put(k,v)."
  Achieving good behavior is hard!
    "Replica" servers are hard to keep identical.
    Clients may crash midway through multi-step update.
    Servers may crash, e.g. after executing but before replying.
    Network partition may make live servers look dead; risk of "split brain".
  Consistency and performance are enemies.
    Strong consistency requires communication, e.g. Get() must check for a recent Put().
    Many designs provide only weak consistency, to gain speed. e.g. Get() does *not* yield the latest Put()!

Topic: **implementation**
    RPC, threads, concurrency control.
    The labs...

## ZooKeeper, Distribute Coordination Service

**What questions does this paper shed light on?**

  * Can we have coordination as a stand-alone general-purpose service?
    What should the API look like?
    How can other distributed applications use it?
  * We paid lots of money for Nx replica servers.
    Can we get Nx performance from them?

**Questions**

* Does this replication arrangement get faster as we add more servers?
* Can replicas serve read-only client requests form their local state?
* Would reads from followers be linearizable? Would reads always yield fresh data?
* What if a client reads from an up-to-date replica, then a lagging replica?

#### Ordering guarantees

* **Linearizable writes**
  clients send writes to the leader
  ==the leader chooses an order, numbered by "zxid"==
  sends to replicas, which all execute in zxid order

* **FIFO client order**
    each client specifies an order for its operations (reads AND writes)
    *writes*: 
      writes appear in the write order in client-specified order (==deleting and creating “ready” file to finish write, atomic==)
    *reads*: 
      each read executes at a particular point in the write order
      a client's successive reads execute at non-decreasing points in the order
      ==a client's read executes after all previous writes by that client==
      a server may block a client's read to wait for previous write, or sync()
    
* **Outcome**
    client may see stale data
    ==client 1 see new data, then client 2 see older data (very helpful for read performance)==

#### Why is ZooKeeper useful despite loose consistency?

* `sync()` causes subsequent client reads to see preceding writes. useful when a read must see latest data

* Writes are well-behaved, e.g. exclusive test-and-set operations, writes really do execute in order, on latest data.

* ==Read order rules ensure "read your own writes". Read order rules help reasoning.==

**A few consequences**:

  Leader must preserve client write order across leader failure.
  Replicas must enforce "a client's reads never go backwards in zxid order" despite replica failure.
  ==Client must track highest zxid it has read to help ensure next read doesn't go backwards even if sent to a different replica.==

#### ZooKeeper API Overview

  the state: a file-system-like tree of *znodes*
  file names, file content, directories, path names
  typical use: configuration info in *znodes*
    set of machines that participate in the application
    which machine is the primary
  each *znode* has a version number
  types of *znodes*:
    regular
    ephemeral
    sequential: name + seqno

**Operations on*znodes***

```go
create(path, data, flags)
    // exclusive -- only first create indicates success
delete(path, version)
    // if znode.version = version, then delete
exists(path, watch)
    // watch=true means also send notification if path is later created/deleted
getData(path, watch)
setData(path, data, version)
    // if znode.version = version, then update
getChildren(path, watch)
sync()
    // sync then read ensures writes before sync are visible to same client's read, client could instead submit a write
```

**ZooKeeper API well tuned to synchronization**:

  + exclusive file creation; exactly one concurrent create returns success
  + `getData()`/`setData(x, version)` supports mini-transactions
  + sessions automate actions when clients fail (e.g. release lock on failure)
  + sequential files create order among multiple clients
  + ==watches -- avoid polling==

**Example: mini-transaction**

while true:
    x, v := getData("f")
    if setData(x + 1, version=v):
        break
==effect is atomic read-modify-write==

**Example: Locks without Herd Effect**

  1. create a "sequential" file
  2. list files
  3. if no lower-numbered, lock is acquired!
  4. if exists(next-lower-numbered, watch=true)
  5.   wait for event...
  6. goto 2

**Using these locks**

  Different from single-machine thread locks!
    If lock holder fails, system automatically releases locks.
    So locks are not really enforcing atomicity of other activities.
    ==To make writes atomic, use "ready" trick or mini-transactions.==
  Useful for master/leader election.
    New leader must inspect state and clean up.


## Chain Replication, CRAQ

#### What is Chain Replication (CR)?

**Goals**: if client gets write reply, data is safe if even one server survives. and linearizable.
  S1, S2, S3, S4
  S1 is "head"
  S4 is "tail"
  **Writes**:
    Client sends to head
    Forwarded down the chain, in order
    Each server overwrites old data with new data
    Tail responds to client
  **Reads**:
    Client sends to tail
    Tail responds (no other nodes involved)

#### Why is CR attractive (vs Raft)?

Client RPCs split between head and tail, vs Raft's leader handles both.
Head sends each write just once, vs Raft's leader sends to all.
Reads involve just one server, not all as in Raft.
Situation after failure simpler than in Raft.

#### Why is it attractive to let clients read any replica in CR?

  The opportunity is that the intermediate replicas may still have spare CPU cycles when the read load is high enough to saturate the tail.
  ==Moving read load from the tail to the intermediate nodes might thus yield higher read throughput on a saturated chain.==
  The CRAQ paper admits there is at least one other way to skin this cat:
  **Split objects over many chains, each server participates in multiple chains.**
    C1: S1 S2 S3
    C2: S2 S3 S1
    C3: S3 S1 S2
  This works if load is more or less evenly divided among chains. It often isn't. Maybe you could divide objects into even more chains. Or use CRAQ's ideas.

#### How does CRAQ support linearizable reads from any replica in the chain?

   (*Figure 2/3, Section 2.3*) Each replica stores a list of versions per object. One clean version, plus dirty version per recent write.
  **Write**:
    Client sends write to head.
    Replicas create new dirty version as write passes through.
    Tail creates clean version, ACKs back along chain, replicas turn "dirty" to "clean".
  **Read from non-tail node**:
    If latest version is clean, reply with that.
    Q: if latest is dirty, why not return most recent clean?
       (newer maybe already exposed to another reader!)
    Q: if latest is dirty, why not return that dirty version?
       (not committed, might disappear due to replica failure)
    ==If dirty, ask tail for latest version number ("**version query**").==

#### CRAQ vs Raft, ZK

* All CRAQ replicas have to participate for any write to commit.
* If a node isn't reachable, CRAQ must wait.
* So not immediately fault-tolerant in the way that Raft and ZK are.
* CRAQ can’t handle **split brain**.

#### How can we safely make use of a replication system that can't handle partition?

==A single "configuration manager" must choose head, chain, tail.==
Everyone (servers, clients) must obey or stop. Regardless of who they locally think is alive/dead.
==A configuration manager is a common and useful pattern. It's the essence of how GFS (master) and VMware-FT (test-and-set server) work.==
Usually Paxos/Raft/ZK for config service, data sharded over many replica groups, CR or something else fast for each replica group.

## Database logging, quorums, Amazon Aurora

* How write-ahead logging interacts with storage systems.
* Quorums for smooth handling of slow/failed replicas.
      Yields great performance improvements.
      But storage isn't general-purpose.
* Tidbits about primary concerns for cloud infrastructure.
      AZ failure, thus need multi-AZ replication.
      Transient slowness of individual replicas.
      Network a worse constraint than CPU or storage.

#### Background

**Amazon EC2 -- cloud computing, aimed at web sites**
you can rent virtual machines -- "instances" run in Amazon's datacenters, on their physical machines; ==virtual local disk stored on directly attached physical disk;== customers run their www servers and DBs on EC2
EC2 is great for www servers: 
    more load -> rent more EC2 instances
    crash -> who cares, stateless, data in DB
EC2 is not ideal for DB e.g. MySQL:
    limited scaling options -- just read/only DB replicas
    limited fault-tolerance; if phys machine dies, disk dies with it, periodic DB backups to Amazon's "S3" bulk storage service

**Amazon EBS (Elastic Block Store)**
block storage that's still available even if an EC2 instance crashes
looks like a disk to EC2 instances, mount e.g. Linux ext4 on EBS volume
implemented as pairs of servers with ==disk drives, chain replication, paxos-based configuration manager==
both replicas are in the same "availability zone" (AZ)
  AZ = machine room or datacenter
  for speed, since all client writes have to wait for both EBS servers
  for a DB on EC2, much better fault-tolerance than locally-attached storage!
  if DB EC2 instance crashes, just re-start it on another EC2 instance and attach to the same EBS volume
note: ==only one EC2 instance can use a given EBS volume at a time, EBS is not shared storage==

**DB-on-EBS shortcomings**
lots of data sent over network -- both log and dirty data pages
      the data pages are big even if only a few bytes are changed
not fault-tolerant enough
      two replicas not enough, but a chain with more would be too slow! both replicas are in same AZ
      vulnerable to fire, flood, power failure, broken Internet connection

**Idea: Amazon's Multi-AZ RDS (Remote Database Services)**
Goal: better fault tolerance by cross-AZ replication
EBS-level mirroring of DB's EBS volume
  Every database server EBS write sent to local EBS and mirror EBS driven by another EC2 instance.
  Mirroring log and all dirty data pages.
DB write has to wait for all 4 EBS replicas to respond.
  Long delays, lots of data. But more fault-tolerant due to 2nd AZ.

#### Aurora Big Picture

1. DB clients (e.g. web servers).
2. One database server (in an EC2 instance).
       Dedicated to one customer.
3. Six storage servers store six replicas of a volume.
       Each volume is private to the one database server.
4. 1 DB server <=> 6 storage servers <=> 3 AZs

* ==Quorum writes for better fault-tolerance without too much waiting.==

* ==Storage servers understand how to apply DB's log to data pages.==

  So only need to send (small) log entries, not (big) dirty pages.
  Sending to many replicas, but not much data.

#### Quorum

**Quorum read/write technique**
goal: fault-tolerant storage, read latest data even if some failures
usually applied to simple read/write (put/get) storage
    N replicas
    writes to any W replicas
    reads from any R replicas
    R + W = N + 1
Q: how to decide which is the most recent copy? max version #
Q: what if a read or write can't assemble a quorum? keep trying

**What is the benefit of quorum read/write storage systems?**
In contrast to e.g. chain replication.
Smooth handling of dead or slow or partitioned storage servers.
Can adjust R and W to make reads or writes faster (but not both).
But:
  Servers not in write quorum must catch up (e.g. Aurora's gossip).
  Can only tolerate min(N-W, N-R) failures.
  Requirement for version numbers makes it most suitable for single writer.

**What does an Aurora quorum write look like?**
  Not a classic quorum write:
    DB server's writes to storage servers do *not* modify existing data items!
  ==A write consists of a new log entry.==
  ==Aurora sends each log record to **all** six storage servers.==
  ==Transaction commit waits for all of xaction's log records to be on a quorum.==
    Stated as VDL >= transaction's last record.
    So every xaction through this one can be recovered after a crash.
  Commit ==> release locks, reply to client.

**Storage Service Design Points**
接收到日志记录，并填入一个内存队列。
在磁盘上将记录持久化，并返回一个应答。
整理记录，并检查是否有因为一些操作丢失导致的记录差异。
通过Gossip和其它节点交流填补差距。
将日志记录合并到新的数据页中去。
定期将日志和新页面备份到S3。
定期进行垃圾回收，清理掉无用的旧版本数据。
定期进行CRC校验。

**What do storage servers do with incoming log entries ("writes")?**
  Just store them, initially.
    As a list of relevant pending log entries per data page.
  So storage server state is
    ==An [old] version of each data page.==
    ==The list of log records required to bring the page up to date.==
  Storage server applies log records in the background. Or when needed by a read.

**When does Aurora use read quorums?**
Only during crash recovery.
    Crash means database server crashed, re-started on a different EC2 instance.
    New database server uses existing six storage servers.
What has to be done for recovery?
    Nothing much for committed transactions -- already on a write quorum.
    Q: What about transactions interrupted by a crash?
    A: Cannot be finished b/c database server has lost its execution state.
DB finds highest log record such that log is complete up to that point.
    Log record exists if it's on any reachable storage server.
    The VCL (Volume Complete LSN) of Section 4.1.
  ==*DB tells storage servers to delete all log entries after the VCL.*==
  DB issues compensating "undo" log entries for transactions that ==started before VCL but didn't commit==.

#### What does an ordinary Aurora read look like?

**LSN**: Log Sequential Number
**VCL**: Volume Complete LSN (highest LSN with availability)
**CPL**: Consistency Point LSN (one CPL per mini-transaction)
**VDL**: Volume Durable LSN (highest CPL && CPL <= VCL, truncate point)
**SCL**: Segment Complete LSN (highest consistent LSN, no cavity)

Not a quorum read!
The Aurora DB server needs a data page, due to a cache miss.
    Writes are log entries; reads yield data pages.
Needs to find a storage server that has all the relevant log entries.
Database server tracks each storage server's SCL (Segment Complete LSN).
    Each storage server has all log entries <= its SCL.
    Reports back to the database server.
==DB server reads from any storage server whose SCL >= highest committed LSN.==
    That storage server may need to apply some log records to the data page.

## Frangipani: A Scalable Distributed File System

**Ideas to remember**

* complex clients sharing simple storage -- maybe scalable
* cache coherence
* distributed transactions
* distributed crash recovery

**Overall design**
  ==a network file system== -> not used for high throughput
  [users; workstations + Frangipani; network; petal]
  Petal: block storage service; replicated; striped+sharded for performance
  What does Frangipani store in Petal?
    ==directories, i-nodes, file content blocks, free bitmaps==
    just like an ordinary hard disk file system
  Frangipani: decentralized file service; cache for performance

**Feature and usage**
environment: single lab with collaborating engineers
common case is exclusive access; want that to be fast; but files sometimes need to be shared; want that to be correct
strong consistency; caching in each workstation -- write-back; most complexity is in clients, not the shared Petal servers

**What's in the Frangipani workstation cache?**
  what if WS1 wants to create and write /grades?
  read / information from Petal into WS1's cache
  add entry for "grades" just in the cache
  don't immediately write back to Petal!

**Challenges**

* **coherence**: will WS2 see WS1’s writes?
* **atomicity**: what if WS1 and WS2 concurrently try to create /a and /b?
* **crash recovery**: what if WS1 crashes while creating a file?

#### Cache Coherence

**goal**: linearizability AND caching.
**cache coherence protocol**: cache coherence is the discipline that ensures that changes in the values of shared operands are propagated throughout the system (i.e. multiple processors) in a timely fashion. (==write through== or ==write back==, see CSAPP)

**Frangipani's coherence protocol (simplified)**
  lock server (LS), with one lock per file/directory

**file owner**
​    x     WS1
​    y     WS2
  workstation (WS) Frangipani cache:

**file/dir lock content**
​    x         busy  ...
​    y         idle  ...
  if WS holds lock,
​    busy: using data right now
​    idle: holds lock but not using the cached data right now
  workstation rules:
​    don't cache unless you hold the lock
​    acquire lock, then read from Petal
​    write to Petal, then release lock
  coherence protocol messages:
​    request (WS -> LS)
​    grant (LS -> WS)
​    revoke (LS -> WS) [another WS ask for]
​    release (WS -> LS)

The locks are named by files/directories (really i-numbers), though the lock server doesn't actually understand anything about file systems or Petal.

#### Atomicity

Frangipani implements transactional file-system operations:
  operation corresponds to a system call (create file, remove file, rename, &c)
  WS acquires locks on all file system data that it will modify, performs operation with all locks held (avoid dead lock)
  ==only releases when finished, only responds to a "revoke" after entire operation is complete==
  thus no other WS can see partially-completed operations, and no other WS can simultaneously perform conflicting updates

Frangipani's locks are doing two different things:
  cache coherence (revealing writes)
  atomic transactions (concealing writes)

#### Crash Recovery

What if a Frangipani workstation dies while holding locks? other workstations will want to continue operating...
What if dead WS had modified data in its cache?
What if dead WS had started to write back modified data to Petal?

Frangipani uses ==write-ahead logging== for crash recovery
  Before writing any of op's cached blocks to Petal, first write log to Petal. So if a crashed workstation has done some Petal writes for an operation, but not all, the writes can be completed from the log in Petal

  1) Frangipani has a separate log for each workstation
     this avoids a logging bottleneck, eases decentralization, but scatters updates to a given file over many logs
  2) Frangipani's logs are in shared Petal storage, not local disk
     WS2 can read WS1's log to recover from WS1 crashing

**What's in the log?**
  log entry (this is a bit of guess-work):
    log sequence number
    array of updates: block #, new version #, addr, new bytes
    just contains meta-data updates, not file content updates
  initially the log entry is in WS local memory (not yet Petal)

**What happens when WS1 crashes while holding locks?**
  Not much, until WS2 requests a lock that WS1 holds
    LS sends revoke to WS1, gets no response
    LS times out, tells WS2 to recover WS1 from its log in Petal
  What does WS2 do to recover from WS1's log?
    Read WS1's log from Petal
    Perform Petal writes described by logged operations
    Tell LS it is done, so LS can release WS1's locks

**Why is it safe to replay just one log, despite interleaved operations on same files by other workstations?**
Example:
  WS1: delete(d/f)               crash
  WS2:              create(d/f)
  WS3:                                  recover WS1
  WS3 is recovering WS1's log -- but it doesn't look at WS2's log
  Will recovery re-play the delete? 
    No -- prevented by **"version number" mechanism**
    Version number in each meta-data block (i-node) in Petal
    Version number(s) in each logged op is block's version plus one
    Recovery replays only if op's version > block version
      i.e. only if the block hasn't yet been updated by this op

**What if WS1 holds a lock while network partition, WS2 decides WS1 is dead, recovers, releases WS1's locks?**
Locks have leases!
    Lock owner can't use a lock past its lease period
    LS doesn't start recovery until after lease expires

**For what workloads is Frangipani likely to have poor performance?**
  lots of read/write sharing?
  lots of small files?

## Distributed Transactions

#### Topics

distributed transactions = concurrency control + atomic commit
concurrency control to provide isolation/serializability
atomic commit to provide atomicity despite of failure

**What is correct behavior for a transaction?**
  usually called "ACID"
    Atomic -- all writes or none, despite failures
    Consistent -- obeys application-specific invariants
    ==Isolated -- no interference between xactions -- serializable==
    Durable -- committed writes are permanent
  we're interested in ACID for distributed transactions
    with data sharded over multiple servers

**Two classes of concurrency control**
  pessimistic:
    lock records before use
    conflicts cause delays (waiting for locks)
  optimistic:
    use records without locking
    commit checks if reads/writes were serializable
    conflict causes abort+retry
    called Optimistic Concurrency Control (OCC)

#### Pessimistic Concurrency Control

"Two-phase locking" is one way to implement serializability
  2PL definition:
    a transaction must acquire a record's lock before using it
    a transaction must hold its locks until *after* commit or abort

**Two-phase lock**
  each database record has a lock
  if distributed, the lock is typically stored at the record's server
  an executing transaction acquires locks as needed, at the first use
    add() and get() implicitly acquires record's lock
    end_xaction() releases all locks
  all locks are exclusive (for this discussion, no reader/writer locks)
  related to thread locking (e.g. Go's Mutex), but easier:
    explicit begin/end_xaction
    DB locks automatically, on first use of each record
    DB unlocks automatically, at transaction end
    DB may automatically abort to cure deadlock

#### Two-Phase Commit

**The setting**
  Data is sharded among multiple servers
  Transactions run on "transaction coordinators" (TCs)
  For each read/write, TC sends RPC to relevant shard server
    Each participant manages locks for its shard of the data
  There may be many concurrent transactions, many TCs
    TC assigns unique transaction ID (TID) to each transaction
    Every message, every table entry tagged with TID to avoid confusion

**Two-phase commit without failures**
  TC sends put(), get(), &c RPCs to A, B.
    ==The modifications are tentative, only to be installed if commit.==
  TC sends PREPARE messages to A and B.
  If A is willing to commit,
    A responds YES, then A is in "prepared" state.
    Otherwise, A responds NO.
  Same for B.
    ==If both A and B say YES, TC sends COMMIT messages to A and B.==
  If either A or B says NO (prepare stage), TC sends ABORT messages.
  A/B commit if they get a COMMIT message from the TC.
    I.e. they write tentative records to the real DB.
    And release the transaction's locks on their records.
    ==A/B acknowledge COMMIT message.==

**Note**
  The commit/abort decision is made by a single entity -- the TC.
  This makes two-phase commit relatively straightforward.
  ==The penalty is that A/B, after voting YES, must wait for the TC.==
    I.e. TC crashes after PREPARE and before COMMIT/ABORT.
    A/B must wait for TC (==in-doubt transactions==).

#### Distributed Transactions Versus Failures

**What if B crashes and restarts?**
  If B sent YES before crash, B must remember (despite crash)!
  Because A might have received a COMMIT and committed.
  So B must be able to commit (or not) even after a reboot.

**Persistent (on-disk) state for participants**
  ==B must remember on disk before saying YES, including modified data.==
  If B reboots, and disk says YES but no COMMIT, B must ask TC, or wait for TC to re-send.
  And meanwhile, B must continue to hold the transaction's locks.
  If TC says COMMIT, B copies modified data to real data.

**What if TC crashes and restarts?**
  ==If TC might have sent COMMIT before crash, TC must remember!==
    Since one worker may already have committed.
  Thus TC must write COMMIT to disk before sending COMMIT msgs.
  And repeat COMMIT if it crashes and reboots,
    or if a participant asks (i.e. if A/B didn't get COMMIT msg).
  ==Participants must filter out duplicate COMMITs (using TID).==

**What if B replied YES to PREPARE, but doesn't receive COMMIT or ABORT?**
  Can B unilaterally decide to abort?
    No! TC might have gotten YES from both, and sent out COMMIT to A, but crashed before sending to B.
    So then A would commit and B would abort: incorrect.
  B can't unilaterally commit, either. A might have voted NO.

**Raft and two-phase commit solve different problems!**
  Use Raft to get high availability by replicating
    i.e. to be able to operate when some servers are crashed
    the servers all do the *same* thing
  Use 2PC when each participant does something different
    And *all* of them must do their part
  ==2PC does not help availability==
    since all servers must be up to get anything done
  ==Raft does not ensure that all servers do something==
    since only a majority have to be alive

## Spanner

**Lessons to learn**

* a rare example of wide-area synchronous replication
* distributed transactions over geographically distributed data
* timestamping scheme
* two-phase commit over Paxos

**Basic organization**
Datacenter A:
    "clients" are web servers e.g. for gmail
    data is sharded over multiple servers:
      a-m
      n-z
Datacenter B:
    has its own local clients, and its own copy of the data shards
      a-m
      n-z
Datacenter C:
    same setup
*Replication is managed by Paxos; one Paxos group per shard; Replicas are in different data centers.*

**What are the challenges?**
*Read of local replica must yield fresh data.*
    But local replica may not reflect latest Paxos writes!
*A transaction may involve multiple shards -> multiple Paxos groups.*
*Transactions that read multiple records must be serializable.*
    But local shards may reflect different subsets of committed transactions!

#### Read/Write Transactions

Summary: two-phase commit (2pc) with Paxos-replicated participants.
  (Omitting timestamps for now.)
  (This is for r/w transactions, not r/o.)
  *Client picks a unique transaction id (TID).*
  Client sends each read to Paxos leader of relevant shard (2.1).
    Each shard first acquires a lock on the relevant record. May have to wait.
    Separate lock table per shard, in shard leader.
    Read locks are not replicated via Paxos, so leader failure -> abort.
  Client keeps writes private until commit.
  When client commits (4.2.1):
    Chooses a Paxos group to act as 2pc Transaction Coordinator (TC).
    Sends writes to relevant shard leaders.
    Each written **shard leader**:
      Acquires lock(s) on the written record(s).
      *Log a "prepare" record via Paxos, to replicate ==lock and new value==.*
      Tell TC it is prepared.
      Or tell TC "no" if crashed and thus lost lock table.
  Transaction Coordinator:
    Decides commit or abort.
    Logs the decision to its group via Paxos.
    Tell participant leaders and client the result.
  Each participant leader:
    Log the TC's decision via Paxos.
    Release the transaction's locks.

* Locking (two-phase locking) ensures serializability.
* Replicating the TC with Paxos solves TC failing with lock held problems.
* Paxos group’s job: 1) Replicates shard data; 2) Replicates two-phase commit state.

#### Read-Only Transactions

Spanner eliminates two big costs for r/o transactions:

1. Read from local replicas, to avoid Paxos and cross-datacenter msgs.
2. No locks, no two-phase commit, no transaction manager.

Correctness constraints on r/o transactions:

1. **Serializable**: Same results as if transactions executed one-by-one, i.e. an r/o xaction must essentially fit between r/w xactions.
2. **Externally consistent**: ==If T1 completes before T2 starts, T2 must see T1's writes.==

**Why not have r/o transactions just read the latest committed values?**

```markdown
    T1:  Wx  Wy  C
    T2:                 Wx  Wy  C
    T3:             Rx             Ry
```

**Snapshot Isolation (SI)**
  Synchronize all computers' clocks.
  Assign every transaction a time-stamp.
    r/w: commit time.
    r/o: start time.
  ==Execute as if one-at-a-time in time-stamp order.==
    Even if actual reads occur in different order.
  Each replica stores multiple time-stamped versions of each record.
    All of a r/w transactions's writes get the same time-stamp.
  An r/o xaction's reads see version as of xaction's time-stamp.
    **The record version with the highest time-stamp less than the r/o xaction's.**
  Called Snapshot Isolation.
  Our example with Snapshot Isolation:

```markdown
                  x@10=9         x@20=8
                  y@10=11        y@20=12
T1 @ 10:  Wx  Wy  C
T2 @ 20:                 Wx  Wy  C
T3 @ 15:             Rx             Ry
```

Now T3's reads will both be served from the @10 versions. T3 won't see T2's write even though T3's read of y occurs after T2.

**Problem**: what if T3 reads x from a replica that hasn't seen T1's write?
**Solution**: replica "safe time".
  Paxos leaders send writes in timestamp order.
  **Before serving a read at time 20, replica must see Paxos write for time > 20.**
    So it knows it has seen all writes < 20.
    What if no write after time 20?
  Must also delay if prepared but uncommitted transactions (Section 4.1.3).

**Problem**: what if clocks are not perfectly synchronized?
**Solution**: Google's time reference system & TrueTime

#### TrueTime

Time service yields a `TTinterval = [earliest, latest]`.
The correct time is guaranteed to be somewhere in the interval.
Interval width computed from measured network delays.
So: server clocks aren't exactly synchronized, but TrueTime provides guaranteed bounds on how wrong a server's clock can be.

**How Spanner ensures that if r/w T1 finishes before r/o T2 starts, TS1 < TS2?**
Two rules (4.1.2):
Start rule:
    xaction TS = TT.now().latest
    ==for r/o, at start time==
    ==for r/w, when commit begins==
Commit wait, for r/w xaction:
    Before commit finishes, delay until TS < TT.now().earliest.
    Guarantees that TS has passed.

```markdown
The scenario is T1 commits, then T2 starts, T2 must see T1's writes.
  r/w T0 @  0: Wx1 C
                   |1-----------10| |11--------------20|
  r/w T1 @ 10:         Wx2 P           C
                                 |10--------12|
  r/o T2 @ 12:                           Rx?
(P for prepare, C for commit)
```

**Why this provides external consistency?**
  Commit wait means r/w TS is guaranteed to be in the past.
  r/o TS = TT.now().latest is guaranteed to be >= correct time (real read happen time).
  Thus r/o TS >= TS of any previous committed transaction (due to its commit wait).
  r/o TS >= correct time >= r/w TS

#### Summary

**Snapshot Isolation gives you serializable r/o transactions.**
    Timestamps set an order.
    Snapshot versions (and safe time) implement consistent reads at a timestamp.
    Transaction sees all writes from lower-TS transactions, none from higher.
    Any number will do for TS if you don't care about external consistency (TS could be wrong).
**Synchronized timestamps yield external consistency.**
    Even among transactions at different data centers.
    Even though reading from local replicas that might lag.

Why is all this useful?
  Fast r/o transactions:
    Read from replica in client's datacenter.
    No locking, no two-phase commit.
  Although:
    ==r/o transaction reads may block due to safe time, to catch up.==
    r/w transaction commits may block in Commit Wait.

## FaRM, OCC

  * works best if few conflicts, due to OCC.
  * data must fit in total RAM.
  * replication only within a datacenter (no geographic distribution).
  * the data model is low-level; would need e.g. SQL library.
  * details driven by specific NIC features; what if NIC had test-and-set?
  * requires somewhat unusual RDMA and NVRAM hardware.

#### Optimistic Concurrency Control

  read objects without locking
  don't install writes until commit
  commit "validates" to see if other xactions conflicted
  valid: commit the writes
  invalid: abort
  called **Optimistic Concurrency Control (OCC)**

**Question: how does FaRM validate?**

**FaRM transaction API (simplified)**
  txCreate()
  o = txRead(oid)  -- RDMA
  o.f += 1
  txWrite(oid, o)  -- purely local
  ok = txCommit()  -- Figure 4

**What's an oid?**
  <region #, address>
  region # indexes a mapping to [ primary, backup1, bachup2, ... ]
  target RDMA NIC uses address directly to read or write RAM

**Server memory layout**
  regions, each an array of objects
  object layout
    header with version #, and lock flag in high bit of version #

#### FaRM’s Transaction

**Execute phase**
  TC (the client, transaction coordinator) reads the objects it needs from servers
    including records that it will write
    using one-sided RDMA reads
    without locking
    this is the optimism in Optimistic Concurrency Control
  TC remembers the version numbers
  TC buffers writes

**LOCK** (first message in commit protocol)
  TC sends to primary of each written object
  TC uses RDMA to append to its log at each primary
  LOCK record contains *oid*, *version #* xaction read, *new value*
  LOCK is now logged in primary's NVRAM, in case power fails

**What does primary do on receipt of LOCK? (commit “validate” in OCC)**
  it polls incoming logs in RAM, sees our LOCK
  ==*if object locked, or version != what xaction read, reply "no"*==
  otherwise set the lock flag and return "yes"
  ==*lock check, version check, and lock set are atomic*==
    using atomic compare-and-swap instructuion
    "locked" flag is high-order bit in version number
    in case other CPU also processing a LOCK, or a client is reading withRDMA
  ==if object already locked, does not block, just replies "no"==
    which will cause the TC to abort the xaction

TC waits for all LOCK reply messages
  if any "no", abort
    append ABORT to primaries' logs so they can release locks
    returns "no" from txCommit()

TC appends COMMIT-PRIMARY to primaries' logs
  TC only waits for RDMA hardware acknowledgement (ack)
    does not wait for primary to process log entry
    hardware ack means safe in primary's NVRAM
  TC returns "yes" from txCommit()

When primary processes COMMIT-PRIMARY in its log:
  copy new value over object's memory
  increment object's version #
  clear object's lock flag

```markdown
example:
T1 and T2 both want to increment x
    x = x + 1
what if T1 and T2 are exactly in step?
  T1: Rx0  Lx  Cx
  T2: Rx0  Lx  Cx               => Lx by CAS, only one will succeed
  what will happen?
or
  T1:    Rx0 Lx Cx
  T2: Rx0          Lx  Cx       => Lx "validate" fails, T2 aborts
or
  T1: Rx0  Lx  Cx
  T2:             Rx0  Lx  Cx
```

**Why FaRM's OCC provides serializability?**
  i.e. checks "was execution same as one at a time?"
  if there was no conflicting transaction:
    the versions won't have changed
  if there was a conflicting transaction:
    one or the other will see a lock or changed version #

**What about VALIDATE in Figure 4?**
  it is an optimization for objects that are just read by a transaction
  ==VALIDATE = one-sided RDMA read to re-fetch object's version # and lock flag==
  if lock set, or version # changed since read, TC aborts
  does not set the lock, thus faster than LOCK+COMMIT

```markdown
VALIDATE example:
x and y initially zero
T1:
  if x == 0:
    y = 1
T2:
  if y == 0:
    x = 1
(this is a classic test example for strong consistency)
T1,T2 yields y=1,x=0
T2,T1 yields x=1,y=0
aborts could leave x=0,y=0
but serializability forbids x=1,y=1

suppose simultaneous:
  T1:  Rx  Ly  Vx  Cy
  T2:  Ry  Lx  Vy  Cx
  *the LOCKs will both succeed, the VALIDATEs will both fail*
how about:
  T1:  Rx  Ly  Vx      Cy
  T2:  Ry          Lx  Vy  Cx
  *T1 commits, T2 aborts*
```

## Scaling Memcache at Facebook

* cautionary tale about not taking consistency seriously from the start
* impressive story of super high capacity from mostly-off-the-shelf software
* fundamental struggle between performance and consistency

**How do web sites cope as they get more users?**
  a typical story of evolution over time:

  1. single machine with web server + application + DB
     DB provides persistent storage, crash recovery, transactions, SQL
     application queries DB, formats HTML, &c
     but: as load grows, application takes too much CPU time
  2. many web FEs, one shared DB
     an easy change, since web server + app already separate from storage
     ==FEs are stateless, all sharing (and concurrency control) via DB==
       stateless -> any FE can serve any request, no harm from FE crash
     but: as load grows, need more FEs, soon single DB server is bottleneck
  3. many web FEs, data sharded over cluster of DBs
     partition data by key over the DBs
       app looks at key (e.g. user), chooses the right DB
     good DB parallelism if no data is super-popular
     painful -- cross-shard transactions and queries probably don't work
       hard to partition too finely
     but: DBs are slow, even for reads, why not cache read requests?
  4. many web FEs, many caches for reads, many DBs for writes
     cost-effective b/c read-heavy and memcached 10x faster than a DB
       memcached just an in-memory hash table, very simple
     complex b/c DB and memcacheds can get out of sync
     fragile b/c cache misses can easily overload the DB
     (next bottleneck will be DB writes -- hard to solve)

**How do FB apps use mc?**
  FB uses mc as a "look-aside" cache
    real data is in the DB
    cached value (if any) should be same as DB
  read:   

```c
    v = get(k) (computes hash(k) to choose mc server)
    if v is nil {
      v = fetch from DB
      set(k, v)
    }
```

  write:

```c
    v = new value
    send k,v to DB
    delete(k)
```

  application determines relationship of mc to DB
    ==mc doesn't know anything about DB==

#### Paper Lessons

* look-aside caching is trickier than it looks -- consistency
* cache is critical:
      not really about reducing user-visible delay
      mostly about shielding DB from huge overload!
* stale reads nevertheless potentially a big headache
      want to avoid unbounded staleness (e.g. missing a delete() entirely)
      ==want read-your-own-writes (read-your-own-writes consistency)==
      more caches -> more sources of staleness
* ==huge "fan-out" => parallel fetch, in-cast congestion==

#### Performance and Regions

  partition: divide keys over mc servers
  replicate: divide clients over mc servers
  partition:
    +++ more memory-efficient (one copy of each k/v)
    +++ works well if no key is very popular
    --- each web server must talk to many mc servers (overhead)
  replication:
    +++ good if a few keys are very popular
    +++ fewer TCP connections
    --- less total data can be cached

Q: what is the point of regions -- multiple complete replicas?
   lower RTT (round-trip time) to users (east coast, west coast)
   quick local reads, from local mc and DB
   (though writes are expensive: must be sent to primary)
   maybe hot replica for main site failure?

Q: why OK performance despite all writes forced to go to the primary region?
   writes are much rarer than reads
   perhaps 100ms to send write to primary, not so bad for human users
   users do not wait for all effects of writes to finish
     i.e. for all stale cached values to be deleted

**Why multiple clusters per region?**
  why not add more and more mc servers to a single cluster?

  1. adding mc servers to cluster doesn't help single popular keys
     replicating (one copy per cluster) does help
  2. more mcs in cluster -> each client request talks to more servers
     and more in-cast congestion at requesting web servers
     client requests fetch 20 to 500 keys! over many mc servers
     MUST request them in parallel (otherwise total latency too large)
     so all replies come back at the same time
     network switches, NIC run out of buffers
  3. hard to build network for single big cluster
     uniform client/server access
     so cross-section b/w must be large -- expensive
     two clusters -> 1/2 the cross-section b/w

**Replicating is a waste of RAM for less-popular items, what to do?**
  =="regional pool"== shared by all clusters
  unpopular objects (no need for many copies)
  the application software decides what to put in regional pool
  frees RAM to replicate more popular objects

**Cold start new mc cluster is a performance problem, what to do?**
  new cluster has 0% hit rate
  if clients use it, will generate big spike in DB load
  ==thus the clients of new cluster first get() from existing cluster==
    and set() into new cluster
    basically lazy copy of existing cluster to new cluster

**Another overload problem: ==thundering herd==**
  one client updates DB and delete()s a key
  lots of clients get() but miss
    they all fetch from DB
    they all set()
  not good: needless DB load
  ==mc gives just the first missing client a "lease" (64-bit token, binding with key)==
    lease = permission to refresh from DB
    ==mc tells others "try get() again in a few milliseconds"==
  effect: only one client reads the DB and does set()
    ==others re-try get() later and hopefully hit==

**What if an mc server fails?**
  can't have DB servers handle the misses -- too much load
  can't shift load to one other mc server -- too much
  can't re-partition all data -- time consuming
  ==Gutter -- pool of idle mc servers, clients only use after mc server fails==
  after a while, failed mc server will be replaced

#### Consistency

**What is their consistency goal?**
  writes go direct to primary DB, ==with transactions==, so writes are consistent
  what about reads?
  reads do not always see the latest write
  ==read-your-own-writes consistency==

**First, how are DB replicas kept consistent across regions?**
  one region is primary
  primary DBs distribute log of updates to DBs in secondary regions
  secondary DBs apply
  secondary DBs are complete replicas (not caches)
  DB replication delay can be considerable (many seconds)

**How do they keep mc content consistent w/ DB content?**
  DBs send ==invalidates== (delete()s) to all mc servers that might cache
  writing client also ==invalidates== mc in local cluster for read-your-own-writes

**What were the races and fixes when multiple clients read from DB and put() into mc?**

Race 1:
  k not in cache
  C1 get(k), misses
  C1 v1 = read k from DB
    C2 writes k = v2 in DB
    C2 delete(k)
  C1 set(k, v1)
  now mc has stale data, delete(k) has already happened
  will stay stale indefinitely, until k is next written
  solved with leases -- C1 gets a lease from mc (NOT from DB), C2's delete() invalidates lease,
    so mc ignores C1's set
    key still missing, so next reader will refresh it from DB

Race 2:
  during cold cluster warm-up
  remember: on miss, clients try get() in warm cluster, copy to cold cluster
  k starts with value v1
  C1 updates k to v2 in DB
  C1 delete(k) -- in cold cluster
  C2 get(k), miss -- in cold cluster
  C2 v1 = get(k) from warm cluster, hits
  C2 set(k, v1) into cold cluster
  now mc has stale v1, but delete() has already happened
    will stay stale indefinitely, until key is next written
  solved with ==two-second hold-off==, just used on cold clusters
    after C1 delete(), cold mc ignores set()s for two seconds
    ==by then, delete() will (probably) propagate via DB to warm cluster==

Race 3:
  k starts with value v1
  C1 is in a secondary region
  C1 updates k=v2 in primary DB
  C1 delete(k) -- local region
  C1 get(k), miss
  C1 read local DB  -- sees v1, not v2!
  later, v2 arrives from primary DB
  ==solved by "remote mark" (read-your-own-writes consistency)==
    C1 delete() marks key "remote" (mc does this "remote" mark)
    get() miss yields "remote"
      tells C1 to read from *primary* region
    "remote" cleared when new data arrives from primary region (DB tells mc?)

**FB/mc lessons for storage system designers?**
  ==cache is vital for throughput survival, not just to reduce latency==
  need flexible tools for controlling partition vs replication
  ==need better ideas for integrating storage layers with consistency==

## Causal Consistency, COPS

**The setting: geo-replication for big web sites**
  multiple datacenters
  each datacenter has a complete copy of all data
  sharded storage servers

**Previous geo-replication solutions**
  **Spanner**
    writes involve Paxos and perhaps two-phase commit
    Paxos quorum for ==write must wait for some remote sites==
    no one site can write on its own
    but has read transactions, consistent, fairly fast
  **Facebook / Memcache**
    ==writes must go to the primary site's MySQL==
    again, non-primary sites cannot write on their own
    but reads are blindingly fast (1,000,000 per second per memcache server)

> *Can we have a system that allows writes from any datacenter?*

#### Straw Man One

three data centers
set of shards in each datacenter
client reads and writes just contact local shard
each shard pushes writes to other datacenters, shard-to-shard, asynchronously
lots of parallelism
this design favors reads
  could instead have writes be purely local, and reads check other datacenters
  or quorum, with overlap, as in Dynamo/Cassandra

==straw man one is an "eventually consistent" design==

  1. clients may see updates in different orders
  2. if no writes for long enough, all clients see same data

> a pretty loose spec, many ways to implement, easy to get good performance
> used in deployed systems, e.g. Dynamo and Cassandra
> but can be tricky for app programmers

```markdown
example app code -- a photo manager:
  C1 uploads photo, adds reference to public list:
    C1: put(photo) put(list)
  C2 reads:
    C2:                       get(list) get(photo)
  C3 also sees new photo, adds to their own list:
    C3: get(list) put(list2)
  C4 sees photo on C3's list:
    C4:                       get(list2) get(photo)
*Q: what can C2 see? what can C4 see?*
```

**How to decide which write is most recent?**
  ==local shard server assigns v#=time when it receives client put()==
  remote datacenter receives put(k, -, v#)
    if v# is larger than version of currently stored value for k
      replace with new value / v#
    otherwise
      ignore new value
  (note "version" is not quite the right word; "timestamp" would be better.)
  wall-clock time almost works!

**What if one datacenter's (or server's) clock is fast by an hour?**
  COPS uses Lamport clocks to assign v#

**If concurrent writes, is it OK to simply discard all but one?**
  the paper's "last-writer-wins"
  sometimes that's OK:
    e.g. there's only a single possible writer, so the problem can't arise
    probably I'm the only person who can write my photo list or profile
  sometimes latest-write-wins is awkward:
    what if put()s are trying to increment a counter?
    or update a shopping cart to have a new item?
  the problem is "conflicting writes"
  ==we'd often like to have a more clever plan to detect and merge==
    real transactions
    mini-transactions -- atomic increment operation, not just get()/put()
    custom conflict resolution for shopping cart (set union?)
  ==resolution of conflicting writes is a problem for eventual/causal consistency==
    ==no single "serialization point" to implement atomic operations or transactions==
  the paper mostly ignores write conflict resolution
    but it's a problem for real systems

#### Straw Man Two

==provide a sync(k, v#) operation==
sync() does not return until:
    every datacenter has at least v# for k
put(k) yields new v# so client can pass it to sync()
you could call this =="eventual plus barriers"==
note sync() is slow: requires talking to / waiting for all datacenters
straw-man-two clients call sync() to force order in which data appears to readers:

```markdown
  C1: v# = put(photo), sync(photo, v#), put(list)
  C2:                                             get(list) get(photo)
  C2 may not see the new list, but if it does, it will see photo too
```

*str*aw man two may not be so bad*
  *it's a straightforward, efficient design*
  *==if you don't need transactions, the semantics are pretty good==*
    *it makes the photo list example work*
    *though requires some thought to get order and sync()s right*
  *read performance is excellent*
  *write performance is OK if you don't write much, or don't mind waiting*
    *after all, the Facebook / Memcache paper says all writes sent to primary datacenter*
    and Spanner writes wait for majority of replica sites*

**Can we have the semantics of sync(), without the cost?**
  a possibility: single write log per datacenter
  each datacenter has a single "log server"
  put() appends to the local server's log, but doesn't wait, no sync()
  log server sends log, in order, to other datacenters
    remote sites apply log in order
    so put(photo), put(list) will appear in that order everywhere
  but the log server might be a bottleneck if there are many shards

#### Straw Man Three

**“Context ”makes causality**
  each ==COPS client== maintains a "context" to reflect order of client ops
  client adds an item to context after each get() and put()
  client tells COPS to order each put() after everything in its context
  get(X)->v2
    context: Xv2
  get(Y)->v4
    context: Xv2, Yv4
  put(Z, -)->v3
    ==client sends Xv2, Yv4 to **shard server** along with new Z==
    context: Xv2, Yv4, Zv3
    ==(COPS optimizes this to just Zv3)==

>  COPS calls a relationship like "Zv3 comes after Yv4" a dependency
>   Yv4 -> Zv3
>   what does a dependency tell COPS to do?
>     if C2 sees Zv3, and then asks for Y, it should see at least Yv4
>   this notion of dependency is meant to match programmer intuition
>     about what it means to get(Y) AND THEN put(Z)
>     or put(Z) AND THEN put(Q)

**What does COPS shard server respond to put()?**
  when it receives a put(Z, -, Yv4) from a client,
    picks a new v# = 3 for Z,
    stores Z, -, v3
    ==sends Z/-/v3/Yv4 to corresponding shard server in each datacenter==
      but does not wait for reply
  remote shard server receives Z/-/v3/Yv4
    talks to local shard server for Y
      waits for Yv4 to arrive
    then updates DB to hold Z/-/v3

**Causal consistency**
  a client establishes dependencies between versions in two ways:
      its own sequence of puts and gets ("Execution Thread")
      reading data written by another client
  dependencies are transitive (if A -> B, and B -> C, then A -> C)

> nice: when updates are causally related, readers see updates 
>   in the order in which the writer saw them
> nice: when updates are NOT causally related, COPS has no order obligations
>
> ```markdown
>   example:
>     C1: put(X)                  put(Z)
>     C2:            put(Y)
> ```
>
>   X must appear before Z (and this requires effort from COPS)
>   ==Y does NOT have to appear before Z==
>   this freedom is the basis of the paper's claim of scalability

**Note: COPS sees only certain causal relationships**
  ones that COPS can observe from client get()s and put()s
  if there are other communication channels, it is only eventually consistent
  ==e.g. I put(k), tell you by phone, you do get(k), maybe you won't see my data==
  ==**COPS isn't externally consistent**==

**Optimizations avoid ever-growing client contexts**

  * put(K)->vN sends context, then clears context, replaces with KvN
    so next put(), e.g. put(L), depends only on KvN
    so remote sites will wait for arrival of KvN before writing L
  * garbage collection sees when all datacenters have a certain version
    that version never needs to be remembered in context
    since it's already visible to everyone

#### COPS-GT Transactions

**How does COPS-GT get_trans() approach work?**
  ==servers store full set of dependencies for each value==
  ==servers store a few old versions==
  get_trans(k1,k2,...)
    ==client library does independent get()s==
    ==get()s return dependencies as well as value/v#==
  client checks dependencies
    for each get() result R,
      for each other get result S mentioned in R's dependencies,
        is Sv#  >= version mentioned in R's dependency?
  if yes for all, can use results
  if no for any, need a second round of get()s for values that were too old
    each fetches the version mentioned in dependencies
    may be old: to avoid cascading dependencies

```markdown
for ACL / list example:
  C1: get_trans(ACL, list)
  C1: get(ACL) -> v1, no deps
      C2: put(ACL, v2)
      C2: put(list, v2, deps=ACL/v2)
  C1: get(list) -> v2, deps: ACL/v2
  (C1 checks dependencies against value versions)
  C1: get(ACL) -> v2
  (now C1 has a causally consistent pair of get() results)
```

**Why are only two phases needed for COPS-GET get transactions?**
  a new value won't be installed until all its dependencies are installed
  so if a get() returns a dependency, it must already be locally installed

**Limitations & Drawbacks, for both COPS and causal consistency**
  *conflicting writes are a serious difficulty*
  *awkward for clients to track causality*
    e.g. user and browser, multiple page views, multiple servers
  *COPS doesn't see external causal dependencies*
  *limited notion of "transaction"*
    only for reads (though later work generalized a bit)
    definition is more subtle than serializable transactions
  *significant overhead to track, communicate, obey causal dependencies*
    remote servers must check and delay updates
    update delays may cascade

**Impact?**
  causal consistency is a popular research idea
    with good reason: promises both performance and useful consistency
  ==causal consistency is rarely used in deployed storage systems==
  what is actually used?
    no geographic replication at all, just local
    primary-site (PNUTS, Facebook/Memcache)
    eventual consistency (Dynamo, Cassandra)
    strongly consistent (Spanner)
